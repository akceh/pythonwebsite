import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from src.agents.agent_coordinator import AgentCoordinator

# Configuration de la page
st.set_page_config(page_title="Analyse des Donn√©es de Risque de Cr√©dit", layout="wide")
st.title("Analyse Exploratoire des Donn√©es de Risque de Cr√©dit")

# Initialisation du coordinateur d'agents
@st.cache_resource
def get_agent_coordinator():
    # Sp√©cifiez ici le chemin vers votre mod√®le GPT4All
    model_path = "models/ggml-gpt4all-j-v1.3-groovy.bin"
    return AgentCoordinator(model_path)

try:
    agent_coordinator = get_agent_coordinator()
    st.sidebar.success("‚úÖ Agents IA initialis√©s avec succ√®s!")
except Exception as e:
    st.sidebar.error(f"‚ùå Erreur lors de l'initialisation des agents: {str(e)}")
    st.sidebar.info("üëâ Assurez-vous d'avoir t√©l√©charg√© le mod√®le GPT4All dans le dossier 'models'")

# Upload du fichier
st.header("Upload des Donn√©es")
uploaded_file = st.file_uploader("Choisissez un fichier CSV", type="csv")

if uploaded_file is not None:
    # Chargement des donn√©es
    try:
        df = pd.read_csv(uploaded_file)
        st.success("‚úÖ Fichier charg√© avec succ√®s!")
        
        # Obtenir les recommandations de l'agent d'analyse
        analysis_recommendations = agent_coordinator.get_analysis_recommendations(df)
        
        # Affichage des informations g√©n√©rales
        st.header("1. Aper√ßu des Donn√©es")
        col1, col2, col3 = st.columns([2, 1, 1])

        with col1:
            st.subheader("Premi√®res lignes du dataset")
            st.dataframe(df.head())

        with col2:
            st.subheader("Dimensions du dataset")
            st.write(f"Nombre de lignes: {df.shape[0]}")
            st.write(f"Nombre de colonnes: {df.shape[1]}")

        with col3:
            st.subheader("Types de donn√©es")
            st.write("Num√©riques:", len(df.select_dtypes(include=['float64', 'int64']).columns))
            st.write("Cat√©gorielles:", len(df.select_dtypes(include=['object']).columns))

        # Information d√©taill√©e sur les colonnes
        st.subheader("Information sur les colonnes")
        info_df = pd.DataFrame({
            'Type de donn√©es': df.dtypes,
            'Valeurs manquantes': df.isnull().sum(),
            'Pourcentage manquant': (df.isnull().sum() / len(df) * 100).round(2),
            'Valeurs uniques': df.nunique()
        })
        st.dataframe(info_df)

        # Obtenir les recommandations de pr√©traitement
        preprocessing_recommendations = agent_coordinator.get_preprocessing_recommendations(
            df, analysis_recommendations
        )

        # Statistiques descriptives
        st.header("2. Statistiques Descriptives")
        st.dataframe(df.describe().round(2))

        # Analyse des variables cat√©gorielles
        st.header("3. Distribution des Variables Cat√©gorielles")
        categorical_cols = df.select_dtypes(include=['object']).columns

        if len(categorical_cols) > 0:
            for col in categorical_cols:
                fig = px.pie(df, names=col, title=f'Distribution de {col}')
                st.plotly_chart(fig)
        else:
            st.info("Aucune variable cat√©gorielle trouv√©e dans le dataset")

        # Analyse des variables num√©riques
        st.header("4. Distribution des Variables Num√©riques")
        numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

        if len(numerical_cols) > 0:
            cols = st.columns(2)
            for idx, col in enumerate(numerical_cols):
                with cols[idx % 2]:
                    fig = px.histogram(df, x=col, title=f'Distribution de {col}')
                    st.plotly_chart(fig)

                    # Statistiques de base
                    stats = df[col].describe()
                    st.write(f"Moyenne: {stats['mean']:.2f}")
                    st.write(f"M√©diane: {stats['50%']:.2f}")
                    st.write(f"√âcart-type: {stats['std']:.2f}")
        else:
            st.info("Aucune variable num√©rique trouv√©e dans le dataset")

        # Matrice de corr√©lation
        st.header("5. Analyse des Corr√©lations")
        numerical_df = df.select_dtypes(include=['float64', 'int64'])
        
        if len(numerical_df.columns) > 1:
            correlation_matrix = numerical_df.corr()

            fig = px.imshow(correlation_matrix,
                          labels=dict(color="Corr√©lation"),
                          title="Matrice de Corr√©lation")
            st.plotly_chart(fig)

            # Identification des corr√©lations fortes
            st.subheader("5.1 Paires de Variables Fortement Corr√©l√©es")
            correlation_pairs = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    if abs(correlation_matrix.iloc[i,j]) > 0.5:  # Seuil de corr√©lation
                        correlation_pairs.append({
                            'Variable 1': correlation_matrix.columns[i],
                            'Variable 2': correlation_matrix.columns[j],
                            'Corr√©lation': correlation_matrix.iloc[i,j]
                        })

            if correlation_pairs:
                correlation_df = pd.DataFrame(correlation_pairs)
                st.dataframe(correlation_df.sort_values('Corr√©lation', key=abs, ascending=False))
                
                st.subheader("Recommandations pour g√©rer la multicolin√©arit√©:")
                st.markdown("""
                Pour les variables fortement corr√©l√©es, voici les approches possibles:
                1. **S√©lection bas√©e sur l'expertise m√©tier**: Choisir la variable la plus pertinente du point de vue business
                2. **Cr√©ation de nouvelles caract√©ristiques**: Combiner les variables corr√©l√©es en une nouvelle caract√©ristique
                3. **Analyse en Composantes Principales (PCA)**: R√©duire la dimensionnalit√© tout en pr√©servant l'information
                4. **√âlimination des variables**: Supprimer la variable la moins importante de chaque paire fortement corr√©l√©e
                """)

                st.subheader("Actions recommand√©es:")
                for pair in correlation_pairs:
                    if abs(pair['Corr√©lation']) > 0.7:
                        st.write(f"- Pour {pair['Variable 1']} et {pair['Variable 2']} (corr√©lation: {pair['Corr√©lation']:.2f}):")
                        if pair['Corr√©lation'] > 0:
                            st.write("  ‚Üí Consid√©rer de n'en garder qu'une seule ou cr√©er un indice composite")
                        else:
                            st.write("  ‚Üí Ces variables ont une forte relation inverse, consid√©rer d'utiliser leur ratio")
            else:
                st.info("Aucune corr√©lation forte trouv√©e entre les variables")
        else:
            st.info("Pas assez de variables num√©riques pour l'analyse de corr√©lation")

        # Analyse de la variable cible
        st.header("6. Analyse de la Variable Cible")
        target_col = st.selectbox("S√©lectionnez la variable cible", df.columns)
        
        if target_col:
            if df[target_col].dtype == 'object' or df[target_col].nunique() < 10:
                target_dist = df[target_col].value_counts()
                fig = px.pie(values=target_dist.values, 
                           names=target_dist.index, 
                           title=f"Distribution de {target_col}")
                st.plotly_chart(fig)
                
                # Relation avec les variables num√©riques
                st.subheader("6.1 Relation avec les Variables Num√©riques")
                numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
                for col in numerical_cols:
                    if col != target_col:
                        fig = px.box(df, x=target_col, y=col, 
                                   title=f'Distribution de {col} par {target_col}')
                        st.plotly_chart(fig)
            else:
                st.warning("La variable cible s√©lectionn√©e n'est pas cat√©gorielle ou a trop de cat√©gories uniques")

        # Bouton pour passer √† l'√©tape suivante
        if st.button("Passer au Preprocessing"):
            st.session_state['current_data'] = df
            st.session_state['target_column'] = target_col
            st.session_state['preprocessing_recommendations'] = preprocessing_recommendations
            st.success("‚úÖ Donn√©es sauvegard√©es! Vous pouvez maintenant lancer le preprocessing.")
            
    except Exception as e:
        st.error(f"Une erreur s'est produite lors du chargement du fichier: {str(e)}")
else:
    st.info("üëÜ Veuillez uploader un fichier CSV pour commencer l'analyse")

st.header("7. Conclusions et Recommandations")
st.markdown("""
### Pr√©paration des donn√©es recommand√©e:
1. **Traitement des valeurs manquantes**
   - Imputation par la m√©diane pour les variables num√©riques
   - Imputation par le mode pour les variables cat√©gorielles

2. **Feature Engineering**
   - Cr√©ation de ratios pertinents
   - Encodage des variables cat√©gorielles

3. **Normalisation**
   - Standardisation des variables num√©riques
   - Mise √† l'√©chelle Min-Max pour certaines caract√©ristiques

4. **S√©lection des caract√©ristiques**
   - √âlimination des variables fortement corr√©l√©es
   - S√©lection bas√©e sur l'importance des caract√©ristiques

### Approches de Mod√©lisation Sugg√©r√©es:
1. **Mod√®les de Base**
   - R√©gression Logistique (baseline)
   - Random Forest
   - XGBoost

2. **Techniques d'Ensemble**
   - Stacking des meilleurs mod√®les
   - Voting Classifier

3. **Validation**
   - Validation crois√©e stratifi√©e
   - M√©triques adapt√©es aux classes d√©s√©quilibr√©es
""")
